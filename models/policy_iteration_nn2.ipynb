{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext Cython\n"
     ]
    }
   ],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "\n",
    "import interface as bb\n",
    "cimport interface as bb\n",
    "\n",
    "import cPickle\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "\n",
    "from libc.math cimport fabs, tanh\n",
    "from scipy.linalg.cython_blas cimport sgemm\n",
    "\n",
    "cimport cython\n",
    "\n",
    "cdef:\n",
    "    int NUM_HIDDEN = 16\n",
    "    float b1[16]\n",
    "    float b2[4]\n",
    "    float min_state[36]\n",
    "    float dif_state[36]\n",
    "\n",
    "cdef float alpha = 1.0, beta = 0.0\n",
    "cdef float[::1,:] s, h, y, w1, w2\n",
    "\n",
    "s = np.empty((1,36), np.float32, order=\"F\")\n",
    "w1 = np.empty((36,NUM_HIDDEN), np.float32, order=\"F\")\n",
    "h = np.empty((1,NUM_HIDDEN), np.float32, order=\"F\")\n",
    "w2 = np.empty((NUM_HIDDEN,4), np.float32, order=\"F\")\n",
    "y = np.empty((1,4), np.float32, order=\"F\")\n",
    "\n",
    "cdef int NUM_CACHE = 21\n",
    "cdef int cache_i = 0, cache_n = 0\n",
    "cdef float[::1,:] cache_s, cache_y\n",
    "\n",
    "cache_s = np.empty((NUM_CACHE,36), np.float32, order=\"F\")\n",
    "cache_y = np.empty((NUM_CACHE,4), np.float32, order=\"F\")\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "cdef void fast_target(float *state, int use_cache = 0):\n",
    "    global cache_i, cache_n\n",
    "    cdef int i, c, m, n, k, lda, ldb, ldc\n",
    "    \n",
    "    if use_cache == 1:\n",
    "        c = 0\n",
    "        while c < cache_n:\n",
    "            i = 0\n",
    "            while i < 36:\n",
    "                if cache_s[c,i] != state[i]:\n",
    "                    break\n",
    "                i += 1\n",
    "            if i == 36:\n",
    "                for i in xrange(4):\n",
    "                    y[0,i] = cache_y[c,i]\n",
    "                return\n",
    "            c += 1\n",
    "        cache_i += 1\n",
    "        if cache_i == NUM_CACHE:\n",
    "            cache_i = 0\n",
    "        if cache_n < NUM_CACHE:\n",
    "            cache_n += 1\n",
    "        for i in xrange(36):\n",
    "            cache_s[cache_i,i] = state[i]\n",
    "            s[0,i] = (state[i] - min_state[i]) / dif_state[i]\n",
    "    else:\n",
    "        for i in xrange(36):\n",
    "            s[0,i] = (state[i] - min_state[i]) / dif_state[i]\n",
    "    \n",
    "    lda = 1\n",
    "    ldb = 36\n",
    "    ldc = 1\n",
    "    m = 1\n",
    "    n = NUM_HIDDEN\n",
    "    k = 36\n",
    "    sgemm(\"N\", \"N\", &m, &n, &k, &alpha, &s[0,0], &lda, &w1[0,0], &ldb, &beta, &h[0,0], &ldc)\n",
    "    \n",
    "    for i in xrange(NUM_HIDDEN):\n",
    "        h[0,i] = tanh(h[0,i] + b1[i])\n",
    "    \n",
    "    lda = 1\n",
    "    ldb = NUM_HIDDEN\n",
    "    ldc = 1\n",
    "    m = 1\n",
    "    n = 4\n",
    "    k = NUM_HIDDEN\n",
    "    sgemm(\"N\", \"N\", &m, &n, &k, &alpha, &h[0,0], &lda, &w2[0,0], &ldb, &beta, &y[0,0], &ldc)\n",
    "    \n",
    "    if use_cache == 1:\n",
    "        for i in xrange(4):\n",
    "            cache_y[cache_i,i] = y[0,i]\n",
    "    \n",
    "\n",
    "@cython.boundscheck(False)\n",
    "cdef int fast_action(float *state, int use_cache = 0):\n",
    "    cdef int i, best_act = -1\n",
    "    cdef float x, best_val = -1e9\n",
    "    fast_target(state, use_cache)\n",
    "    for i in xrange(4):\n",
    "        x = y[0,i] + b2[i]\n",
    "        if x > best_val:\n",
    "            best_val = x\n",
    "            best_act = i\n",
    "    return best_act\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "cdef float fast_value(float *state):\n",
    "    cdef int i\n",
    "    cdef float x, best_val = -1e9\n",
    "    fast_target(state, 1)\n",
    "    for i in xrange(4):\n",
    "        x = y[0,i] + b2[i]\n",
    "        if x > best_val:\n",
    "            best_val = x\n",
    "    return best_val\n",
    "\n",
    "\n",
    "def dump_embedding():\n",
    "    cdef int i\n",
    "    global min_state, dif_state\n",
    "    state_min = None\n",
    "    state_dif = None\n",
    "    with open('state36.pkl', 'r') as file:\n",
    "        embedding = cPickle.load(file)\n",
    "        state_min = embedding['state_min']\n",
    "        state_dif = embedding['state_dif']\n",
    "        for i in xrange(36):\n",
    "            min_state[i] = state_min[i]\n",
    "            dif_state[i] = state_dif[i]\n",
    "        print 'Embedding:', state_min.mean(), state_dif.mean()\n",
    "    return state_min, state_dif\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "def dump_weights(weights):\n",
    "    cdef int i, j\n",
    "    for i in xrange(NUM_HIDDEN):\n",
    "        for j in xrange(36):\n",
    "            w1[j,i] = weights[0][j,i]\n",
    "        b1[i] = weights[1][i]\n",
    "    for i in xrange(4):\n",
    "        for j in xrange(NUM_HIDDEN):\n",
    "            w2[j,i] = weights[2][j,i]\n",
    "        b2[i] = weights[3][i]\n",
    "\n",
    "\n",
    "def prepare_bbox(level='train', verbose=0):\n",
    "    if bb.is_level_loaded():\n",
    "        bb.reset_level()\n",
    "    bb.load_level('../levels/'+level+'_level.data', verbose)\n",
    "\n",
    "cdef float _rewards[4]\n",
    "cdef float _mask[4]\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "cdef crollout(int epoch=0, float curriculum=0.7):\n",
    "    cdef:\n",
    "        int i, a, action, has_next, checkpoint_id, has_change\n",
    "        float r, prev_score, init_state35, next_state35, next_state35_abs, prev_state35_abs\n",
    "        float *state\n",
    "    \n",
    "    init_state35 = bb.c_get_state()[35]\n",
    "    checkpoint_id = bb.create_checkpoint()\n",
    "    for a in xrange(4):\n",
    "        _rewards[a] = 0\n",
    "        _mask[a] = 0\n",
    "        \n",
    "        prev_score = bb.c_get_score()\n",
    "        has_next = bb.c_do_action(a)\n",
    "        state = bb.c_get_state()\n",
    "        next_state35 = state[35]  \n",
    "        \n",
    "        if init_state35 != next_state35 or np.random.rand() < curriculum:\n",
    "            r = bb.c_get_score() - prev_score\n",
    "            #next_state35_abs = fabs(next_state35)\n",
    "            #if next_state35_abs > 0.75 and next_state35_abs > fabs(init_state35):\n",
    "            #    r -= next_state35_abs\n",
    "            #prev_state35_abs = next_state35_abs\n",
    "            prev_score = bb.c_get_score()\n",
    "            if has_next == 1:\n",
    "                for i in xrange(20):\n",
    "                    if epoch > 0:\n",
    "                        action = fast_action(state, 1)\n",
    "                    else:\n",
    "                        action = 3\n",
    "                    has_next = bb.c_do_action(action)\n",
    "                    r += bb.c_get_score() - prev_score\n",
    "                    state = bb.c_get_state()\n",
    "                    #next_state35_abs = fabs(state[35])\n",
    "                    #if next_state35_abs > 0.75 and next_state35_abs > prev_state35_abs:\n",
    "                    #    r -= next_state35_abs\n",
    "                    #prev_state35_abs = next_state35_abs\n",
    "                    prev_score = bb.c_get_score()\n",
    "                    if has_next == 0:\n",
    "                        break\n",
    "                if has_next == 1 and epoch > 0:\n",
    "                    r += fast_value(state)\n",
    "            \n",
    "            _rewards[a] = r\n",
    "            _mask[a] = 1\n",
    "        \n",
    "        bb.load_from_checkpoint(checkpoint_id)\n",
    "    bb.clear_all_checkpoints()\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "def rollout(epoch=0, curriculum=0.7):\n",
    "    cdef int i\n",
    "    crollout(epoch, curriculum)\n",
    "    rewards = np.empty(4, dtype=np.float32)\n",
    "    mask = np.empty(4, dtype=np.float32)\n",
    "    for i in xrange(4):\n",
    "        rewards[i] = _rewards[i]\n",
    "        mask[i] = _mask[i]\n",
    "    if (rewards == 0).all():\n",
    "        return None, None\n",
    "    return rewards, mask\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def build_model():\n",
    "    \n",
    "    l_input = lasagne.layers.InputLayer((None, 36))\n",
    "    l_input = lasagne.layers.GaussianNoiseLayer(l_input, sigma=0.01)\n",
    "    l_hidden = lasagne.layers.DenseLayer(\n",
    "        l_input,\n",
    "        num_units=NUM_HIDDEN,\n",
    "        W=lasagne.init.Normal(),\n",
    "        nonlinearity=lasagne.nonlinearities.tanh\n",
    "    )\n",
    "    l_out = lasagne.layers.GaussianNoiseLayer(l_hidden, sigma=0.01)\n",
    "    #l_out = lasagne.layers.DropoutLayer(l_out, p=0.5)\n",
    "    l_out = lasagne.layers.DenseLayer(\n",
    "        l_out,\n",
    "        num_units=4,\n",
    "        W=lasagne.init.Normal(),\n",
    "        nonlinearity=None\n",
    "    )\n",
    "\n",
    "    state = T.matrix('state')\n",
    "    target = T.matrix('target')\n",
    "    mask = T.matrix('mask')\n",
    "\n",
    "    prediction = lasagne.layers.get_output(l_out, state)\n",
    "    loss = T.mean(mask * (prediction - target)**2)\n",
    "    #l2_penalty = T.sum(l_hidden.W**2) + T.sum(l_out.W**2)\n",
    "    #loss = loss + 0.00001 * l2_penalty\n",
    "    params = lasagne.layers.helper.get_all_params(l_out)\n",
    "    grads = T.grad(loss, params)\n",
    "    #updates = lasagne.updates.rmsprop(grads, params, .0001, .95)\n",
    "    updates = lasagne.updates.adam(grads, params, .0001)\n",
    "    train = theano.function([state, target, mask], loss, updates=updates)\n",
    "    grads = [T.sqrt(T.sum(g**2)) for g in grads]\n",
    "    debug = theano.function([state, target, mask], grads)\n",
    "    \n",
    "    return l_out, train, debug\n",
    "\n",
    "\n",
    "@cython.boundscheck(False)\n",
    "def policy_iteration(n_epochs=20):\n",
    "    cdef int epoch, action\n",
    "    \n",
    "    best_score = 3000\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    M = []\n",
    "    \n",
    "    curriculum = 0.3\n",
    "    curriculum_max = 0.6\n",
    "    curriculum_inc = (curriculum_max - curriculum) / 10.0\n",
    "    \n",
    "    state_min, state_dif = dump_embedding()\n",
    "    model, train, debug = build_model()\n",
    "    weights_out = []\n",
    "    \n",
    "    def train_epoch(X, Y, M):\n",
    "        cdef float loss = 0\n",
    "        cdef int i = 0, e = min(5, 2 + epoch)\n",
    "        for _ in xrange(e):\n",
    "            N = X.shape[0]\n",
    "            I = np.random.permutation(N)\n",
    "            X = X[I]\n",
    "            Y = Y[I]\n",
    "            M = M[I]\n",
    "            N = int(N / BATCH_SIZE)\n",
    "            for b in xrange(N):\n",
    "                Xb = X[b*BATCH_SIZE:(b+1)*BATCH_SIZE]\n",
    "                Yb = Y[b*BATCH_SIZE:(b+1)*BATCH_SIZE]\n",
    "                Mb = M[b*BATCH_SIZE:(b+1)*BATCH_SIZE]\n",
    "                loss += train(Xb, Yb, Mb)\n",
    "                i += 1\n",
    "                if np.isnan(loss):\n",
    "                    raise TypeError(\"Loss function return NaN!\")\n",
    "\n",
    "        W = lasagne.layers.get_all_param_values(model)\n",
    "        \n",
    "        G = debug(Xb, Yb, Mb)\n",
    "        for w, g in zip(W, G):\n",
    "            print \"{:10s} \\t {:8.5f} \\t {:8.5f}\".format(w.shape, np.sqrt((w**2).sum()), float(g))\n",
    "\n",
    "        print \"Updates {}, Loss = {:.4f}\".format(i, loss / i)\n",
    "        \n",
    "        return W\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        prepare_bbox('train')\n",
    "        while True:\n",
    "            \n",
    "            rewards, mask = rollout(epoch, curriculum)\n",
    "            if rewards is not None:\n",
    "                state = (bb.get_state() - state_min) / state_dif\n",
    "                X.append(state)\n",
    "                Y.append(rewards)\n",
    "                M.append(mask)\n",
    "            \n",
    "            if epoch > 0:\n",
    "                action = fast_action(bb.c_get_state(), 1)\n",
    "            else:\n",
    "                action = np.random.randint(4)\n",
    "\n",
    "            if bb.c_do_action(action) == 0:\n",
    "                train_score = bb.finish(verbose=0)\n",
    "                break\n",
    "            \n",
    "        Xnew = np.array(X).astype(np.float32)\n",
    "        Ynew = np.array(Y).astype(np.float32)\n",
    "        Mnew = np.array(M).astype(np.float32)\n",
    "\n",
    "        del X[:]\n",
    "        del Y[:]\n",
    "        del M[:]\n",
    "\n",
    "        if epoch > 0:\n",
    "            Xa = np.concatenate([Xnew, Xold], axis=0)\n",
    "            Ya = np.concatenate([Ynew, Yold], axis=0)\n",
    "            Ma = np.concatenate([Mnew, Mold], axis=0)\n",
    "        else:\n",
    "            Xa = Xnew\n",
    "            Ya = Ynew\n",
    "            Ma = Mnew\n",
    "\n",
    "        Xold = Xnew\n",
    "        Yold = Ynew\n",
    "        Mold = Mnew\n",
    "\n",
    "        weights = train_epoch(Xa, Ya, Ma)\n",
    "        weights_out.append(weights)\n",
    "        \n",
    "        print 'Epoch: {}, sample prob: {}, time: {}'.format(epoch, curriculum, int(time.time() - start))\n",
    "        test_score, _ = test(weights)\n",
    "        print\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        dump_weights(weights)\n",
    "        \n",
    "        if test_score > best_score:\n",
    "            with open('best_weights' + str(NUM_HIDDEN) + '.pkl', 'w') as f:\n",
    "                cPickle.dump(weights, f)\n",
    "            best_score = test_score\n",
    "            \n",
    "        curriculum = min(curriculum_max, curriculum + curriculum_inc)\n",
    "    \n",
    "    return weights_out\n",
    "\n",
    "def test(weights):\n",
    "    cdef:\n",
    "        int action, has_next\n",
    "    \n",
    "    dump_weights(weights)\n",
    "    results = []\n",
    "    for lvl in  ('train', 'test'):\n",
    "        prepare_bbox(lvl)\n",
    "        has_next = 1\n",
    "        while has_next:\n",
    "            action = fast_action(bb.c_get_state(), 0)\n",
    "            has_next = bb.c_do_action(action)\n",
    "        results.append(bb.finish(verbose=0))\n",
    "    print 'average  {:.2f}, test {:.2f}, train {:.2f}'.format(0.5*sum(results), results[1], results[0])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding: -5.73218 9.42407\n",
      "(36, 16)   \t  0.90372 \t  0.68124\n",
      "(16,)      \t  0.08046 \t  0.19938\n",
      "(16, 4)    \t  0.49508 \t  0.61496\n",
      "(4,)       \t  0.04463 \t  1.05380\n",
      "Updates 27440, Loss = 14.9823\n",
      "Epoch: 0, c: 0.3, time: 40\n",
      "average  19.51, test 312.67, train -273.65\n",
      "\n",
      "(36, 16)   \t  2.55809 \t  0.44457\n",
      "(16,)      \t  0.25087 \t  0.11353\n",
      "(16, 4)    \t  1.40660 \t  0.35460\n",
      "(4,)       \t  0.08820 \t  0.17449\n",
      "Updates 87507, Loss = 7.5495\n",
      "Epoch: 1, c: 0.33, time: 104\n",
      "average  -3365.76, test -3571.39, train -3160.14\n",
      "\n",
      "(36, 16)   \t  4.18970 \t  0.08154\n",
      "(16,)      \t  0.44802 \t  0.01658\n",
      "(16, 4)    \t  1.91879 \t  0.07149\n",
      "(4,)       \t  0.07196 \t  0.02623\n",
      "Updates 125952, Loss = 0.6672\n",
      "Epoch: 2, c: 0.36, time: 156\n",
      "average  -410.76, test -324.98, train -496.53\n",
      "\n",
      "(36, 16)   \t  5.63463 \t  0.24911\n",
      "(16,)      \t  0.75491 \t  0.06929\n",
      "(16, 4)    \t  2.68036 \t  0.10960\n",
      "(4,)       \t  0.10697 \t  0.02414\n",
      "Updates 162935, Loss = 0.3198\n",
      "Epoch: 3, c: 0.39, time: 191\n",
      "average  951.20, test 1027.20, train 875.21\n",
      "\n",
      "(36, 16)   \t  6.64459 \t  0.29123\n",
      "(16,)      \t  0.89328 \t  0.08325\n",
      "(16, 4)    \t  3.42290 \t  0.14434\n",
      "(4,)       \t  0.11488 \t  0.05564\n",
      "Updates 167715, Loss = 0.2421\n",
      "Epoch: 4, c: 0.42, time: 178\n",
      "average  856.16, test 942.58, train 769.73\n",
      "\n",
      "(36, 16)   \t  7.74301 \t  0.15318\n",
      "(16,)      \t  1.07884 \t  0.04021\n",
      "(16, 4)    \t  4.08138 \t  0.10147\n",
      "(4,)       \t  0.11885 \t  0.03723\n",
      "Updates 171870, Loss = 0.2301\n",
      "Epoch: 5, c: 0.45, time: 284\n",
      "average  1551.86, test 1678.06, train 1425.65\n",
      "\n",
      "(36, 16)   \t  8.58290 \t  0.48728\n",
      "(16,)      \t  1.26638 \t  0.13668\n",
      "(16, 4)    \t  4.54668 \t  0.23045\n",
      "(4,)       \t  0.13446 \t  0.08009\n",
      "Updates 175345, Loss = 0.2408\n",
      "Epoch: 6, c: 0.48, time: 179\n",
      "average  1771.60, test 1769.76, train 1773.43\n",
      "\n",
      "(36, 16)   \t  9.25802 \t  0.57194\n",
      "(16,)      \t  1.44266 \t  0.16794\n",
      "(16, 4)    \t  4.91820 \t  0.25323\n",
      "(4,)       \t  0.14035 \t  0.08626\n",
      "Updates 178285, Loss = 0.2625\n",
      "Epoch: 7, c: 0.51, time: 204\n",
      "average  1774.89, test 1944.35, train 1605.43\n",
      "\n",
      "(36, 16)   \t  9.89076 \t  0.36201\n",
      "(16,)      \t  1.59475 \t  0.09801\n",
      "(16, 4)    \t  5.24220 \t  0.12259\n",
      "(4,)       \t  0.13072 \t  0.03815\n",
      "Updates 180795, Loss = 0.3134\n",
      "Epoch: 8, c: 0.54, time: 186\n",
      "average  1634.33, test 1709.59, train 1559.06\n",
      "\n",
      "(36, 16)   \t 10.46353 \t  0.69157\n",
      "(16,)      \t  1.73955 \t  0.18509\n",
      "(16, 4)    \t  5.60010 \t  0.21204\n",
      "(4,)       \t  0.12860 \t  0.05928\n",
      "Updates 182850, Loss = 0.4207\n",
      "Epoch: 9, c: 0.57, time: 191\n",
      "average  2315.14, test 2364.78, train 2265.49\n",
      "\n",
      "(36, 16)   \t 11.07013 \t  1.57946\n",
      "(16,)      \t  1.86762 \t  0.49020\n",
      "(16, 4)    \t  5.99282 \t  0.63372\n",
      "(4,)       \t  0.12505 \t  0.20868\n",
      "Updates 184555, Loss = 0.5503\n",
      "Epoch: 10, c: 0.6, time: 301\n",
      "average  2718.01, test 2932.63, train 2503.38\n",
      "\n",
      "(36, 16)   \t 11.67336 \t  0.58488\n",
      "(16,)      \t  1.99598 \t  0.16821\n",
      "(16, 4)    \t  6.37471 \t  0.29723\n",
      "(4,)       \t  0.12279 \t  0.09543\n",
      "Updates 185320, Loss = 0.6461\n",
      "Epoch: 11, c: 0.6, time: 193\n",
      "average  2609.04, test 2691.24, train 2526.84\n",
      "\n",
      "(36, 16)   \t 12.23643 \t  0.97204\n",
      "(16,)      \t  2.12018 \t  0.27978\n",
      "(16, 4)    \t  6.74756 \t  0.39741\n",
      "(4,)       \t  0.12507 \t  0.12592\n",
      "Updates 185305, Loss = 0.7012\n",
      "Epoch: 12, c: 0.6, time: 287\n",
      "average  2739.11, test 2913.95, train 2564.28\n",
      "\n",
      "(36, 16)   \t 12.78956 \t  1.33309\n",
      "(16,)      \t  2.23817 \t  0.36618\n",
      "(16, 4)    \t  7.10290 \t  0.36960\n",
      "(4,)       \t  0.13581 \t  0.12430\n",
      "Updates 185300, Loss = 0.7384\n",
      "Epoch: 13, c: 0.6, time: 193\n",
      "average  2538.15, test 2675.91, train 2400.39\n",
      "\n",
      "(36, 16)   \t 13.30069 \t  0.51704\n",
      "(16,)      \t  2.34995 \t  0.12528\n",
      "(16, 4)    \t  7.47120 \t  0.14316\n",
      "(4,)       \t  0.15860 \t  0.04092\n",
      "Updates 185285, Loss = 0.8182\n",
      "Epoch: 14, c: 0.6, time: 191\n",
      "average  2471.45, test 2543.74, train 2399.15\n",
      "\n",
      "(36, 16)   \t 13.77601 \t  1.10011\n",
      "(16,)      \t  2.45981 \t  0.31589\n",
      "(16, 4)    \t  7.84714 \t  0.21487\n",
      "(4,)       \t  0.17783 \t  0.06412\n",
      "Updates 185285, Loss = 0.8314\n",
      "Epoch: 15, c: 0.6, time: 237\n",
      "average  2696.38, test 2812.47, train 2580.29\n",
      "\n",
      "(36, 16)   \t 14.21875 \t  2.13666\n",
      "(16,)      \t  2.56515 \t  0.57705\n",
      "(16, 4)    \t  8.19424 \t  0.66281\n",
      "(4,)       \t  0.19290 \t  0.20790\n",
      "Updates 185315, Loss = 0.8368\n",
      "Epoch: 16, c: 0.6, time: 194\n",
      "average  2627.30, test 2767.25, train 2487.34\n",
      "\n",
      "(36, 16)   \t 14.64335 \t  0.76831\n",
      "(16,)      \t  2.66956 \t  0.18694\n",
      "(16, 4)    \t  8.53003 \t  0.26951\n",
      "(4,)       \t  0.21092 \t  0.08715\n",
      "Updates 185310, Loss = 0.9262\n",
      "Epoch: 17, c: 0.6, time: 300\n",
      "average  2496.33, test 2643.31, train 2349.36\n",
      "\n",
      "(36, 16)   \t 15.01278 \t  1.46626\n",
      "(16,)      \t  2.78272 \t  0.42036\n",
      "(16, 4)    \t  8.85963 \t  0.45592\n",
      "(4,)       \t  0.21747 \t  0.14440\n",
      "Updates 185315, Loss = 0.9720\n",
      "Epoch: 18, c: 0.6, time: 193\n",
      "average  2691.44, test 2812.85, train 2570.03\n",
      "\n",
      "(36, 16)   \t 15.33583 \t  0.50245\n",
      "(16,)      \t  2.89907 \t  0.08968\n",
      "(16, 4)    \t  9.14380 \t  0.28220\n",
      "(4,)       \t  0.22453 \t  0.08844\n",
      "Updates 185295, Loss = 0.9105\n",
      "Epoch: 19, c: 0.6, time: 227\n",
      "average  1868.19, test 1935.78, train 1800.59\n",
      "\n",
      "(36, 16)   \t 15.64759 \t  1.90420\n",
      "(16,)      \t  3.01979 \t  0.51530\n",
      "(16, 4)    \t  9.38480 \t  0.35039\n",
      "(4,)       \t  0.23492 \t  0.11429\n",
      "Updates 185290, Loss = 0.9092\n",
      "Epoch: 20, c: 0.6, time: 192\n",
      "average  2398.02, test 2572.09, train 2223.95\n",
      "\n",
      "(36, 16)   \t 15.96257 \t  0.76534\n",
      "(16,)      \t  3.12843 \t  0.21081\n",
      "(16, 4)    \t  9.62904 \t  0.22269\n",
      "(4,)       \t  0.23412 \t  0.06390\n",
      "Updates 185320, Loss = 0.9554\n",
      "Epoch: 21, c: 0.6, time: 192\n",
      "average  2479.20, test 2611.98, train 2346.42\n",
      "\n",
      "(36, 16)   \t 16.24975 \t  1.29115\n",
      "(16,)      \t  3.22607 \t  0.35692\n",
      "(16, 4)    \t  9.84506 \t  0.26781\n",
      "(4,)       \t  0.23804 \t  0.08596\n",
      "Updates 185305, Loss = 0.9619\n",
      "Epoch: 22, c: 0.6, time: 300\n",
      "average  2523.60, test 2690.00, train 2357.21\n",
      "\n",
      "(36, 16)   \t 16.50017 \t  0.60296\n",
      "(16,)      \t  3.32512 \t  0.14586\n",
      "(16, 4)    \t  9.98872 \t  0.15862\n",
      "(4,)       \t  0.24940 \t  0.04146\n",
      "Updates 185285, Loss = 0.8240\n",
      "Epoch: 23, c: 0.6, time: 192\n",
      "average  2582.44, test 2737.55, train 2427.33\n",
      "\n",
      "(36, 16)   \t 16.76345 \t  0.85961\n",
      "(16,)      \t  3.41673 \t  0.24074\n",
      "(16, 4)    \t 10.10120 \t  0.19628\n",
      "(4,)       \t  0.25924 \t  0.06089\n",
      "Updates 185305, Loss = 0.7328\n",
      "Epoch: 24, c: 0.6, time: 281\n",
      "average  2752.26, test 2943.00, train 2561.53\n",
      "\n",
      "(36, 16)   \t 17.04276 \t  0.71793\n",
      "(16,)      \t  3.51157 \t  0.18350\n",
      "(16, 4)    \t 10.26314 \t  0.28635\n",
      "(4,)       \t  0.27927 \t  0.08368\n",
      "Updates 185300, Loss = 0.7833\n",
      "Epoch: 25, c: 0.6, time: 192\n",
      "average  2689.73, test 2821.09, train 2558.37\n",
      "\n",
      "(36, 16)   \t 17.30467 \t  1.86463\n",
      "(16,)      \t  3.60461 \t  0.52134\n",
      "(16, 4)    \t 10.43393 \t  0.45938\n",
      "(4,)       \t  0.30352 \t  0.14530\n",
      "Updates 185290, Loss = 0.7732\n",
      "Epoch: 26, c: 0.6, time: 194\n",
      "average  2557.03, test 2682.65, train 2431.41\n",
      "\n",
      "(36, 16)   \t 17.56704 \t  1.03759\n",
      "(16,)      \t  3.68910 \t  0.30011\n",
      "(16, 4)    \t 10.59744 \t  0.29138\n",
      "(4,)       \t  0.32555 \t  0.09010\n",
      "Updates 185295, Loss = 0.8060\n",
      "Epoch: 27, c: 0.6, time: 257\n",
      "average  2339.29, test 2431.52, train 2247.06\n",
      "\n",
      "(36, 16)   \t 17.80770 \t  1.73929\n",
      "(16,)      \t  3.77486 \t  0.50562\n",
      "(16, 4)    \t 10.75041 \t  0.52134\n",
      "(4,)       \t  0.34644 \t  0.17206\n",
      "Updates 185300, Loss = 0.8467\n",
      "Epoch: 28, c: 0.6, time: 193\n",
      "average  2540.44, test 2635.94, train 2444.94\n",
      "\n",
      "(36, 16)   \t 18.03172 \t  1.89937\n",
      "(16,)      \t  3.85760 \t  0.54418\n",
      "(16, 4)    \t 10.86878 \t  0.44069\n",
      "(4,)       \t  0.36702 \t  0.13771\n",
      "Updates 185325, Loss = 0.7693\n",
      "Epoch: 29, c: 0.6, time: 303\n",
      "average  2636.78, test 2776.56, train 2497.00\n",
      "\n",
      "(36, 16)   \t 18.25772 \t  0.48154\n",
      "(16,)      \t  3.93881 \t  0.10253\n",
      "(16, 4)    \t 10.95069 \t  0.20658\n",
      "(4,)       \t  0.38971 \t  0.06092\n",
      "Updates 185315, Loss = 0.6557\n",
      "Epoch: 30, c: 0.6, time: 192\n",
      "average  2676.56, test 2754.99, train 2598.12\n",
      "\n",
      "(36, 16)   \t 18.47612 \t  1.45850\n",
      "(16,)      \t  4.00856 \t  0.42515\n",
      "(16, 4)    \t 11.01877 \t  0.39055\n",
      "(4,)       \t  0.41328 \t  0.11826\n",
      "Updates 185305, Loss = 0.6530\n",
      "Epoch: 31, c: 0.6, time: 208\n",
      "average  2254.88, test 2424.21, train 2085.55\n",
      "\n",
      "(36, 16)   \t 18.68825 \t  0.93774\n",
      "(16,)      \t  4.06996 \t  0.22729\n",
      "(16, 4)    \t 11.08118 \t  0.18644\n",
      "(4,)       \t  0.43462 \t  0.05529\n",
      "Updates 185320, Loss = 0.6472\n",
      "Epoch: 32, c: 0.6, time: 192\n",
      "average  2079.41, test 2086.25, train 2072.58\n",
      "\n",
      "(36, 16)   \t 18.91154 \t  0.83059\n",
      "(16,)      \t  4.12030 \t  0.20232\n",
      "(16, 4)    \t 11.15300 \t  0.22165\n",
      "(4,)       \t  0.45659 \t  0.06763\n",
      "Updates 185310, Loss = 0.6764\n",
      "Epoch: 33, c: 0.6, time: 191\n",
      "average  2473.51, test 2641.19, train 2305.82\n",
      "\n",
      "(36, 16)   \t 19.12010 \t  0.57661\n",
      "(16,)      \t  4.18519 \t  0.15576\n",
      "(16, 4)    \t 11.21714 \t  0.29174\n",
      "(4,)       \t  0.48208 \t  0.08548\n",
      "Updates 185300, Loss = 0.6500\n",
      "Epoch: 34, c: 0.6, time: 301\n",
      "average  2749.78, test 2859.15, train 2640.42\n",
      "\n",
      "(36, 16)   \t 19.31308 \t  0.92490\n",
      "(16,)      \t  4.26424 \t  0.25610\n",
      "(16, 4)    \t 11.28213 \t  0.34237\n",
      "(4,)       \t  0.51903 \t  0.10059\n",
      "Updates 185305, Loss = 0.6028\n",
      "Epoch: 35, c: 0.6, time: 191\n",
      "average  2666.41, test 2784.92, train 2547.89\n",
      "\n",
      "(36, 16)   \t 19.53022 \t  0.94252\n",
      "(16,)      \t  4.33137 \t  0.25188\n",
      "(16, 4)    \t 11.37156 \t  0.29777\n",
      "(4,)       \t  0.55719 \t  0.09193\n",
      "Updates 185310, Loss = 0.5973\n",
      "Epoch: 36, c: 0.6, time: 291\n",
      "average  2675.93, test 2838.42, train 2513.45\n",
      "\n",
      "(36, 16)   \t 19.75335 \t  0.39880\n",
      "(16,)      \t  4.38424 \t  0.08659\n",
      "(16, 4)    \t 11.47129 \t  0.12969\n",
      "(4,)       \t  0.59918 \t  0.03428\n",
      "Updates 185320, Loss = 0.5766\n",
      "Epoch: 37, c: 0.6, time: 192\n",
      "average  2725.80, test 2868.21, train 2583.40\n",
      "\n",
      "(36, 16)   \t 19.94302 \t  0.90105\n",
      "(16,)      \t  4.45967 \t  0.23408\n",
      "(16, 4)    \t 11.56972 \t  0.21392\n",
      "(4,)       \t  0.64449 \t  0.06400\n",
      "Updates 185325, Loss = 0.5680\n",
      "Epoch: 38, c: 0.6, time: 192\n",
      "average  2630.73, test 2806.98, train 2454.48\n",
      "\n",
      "(36, 16)   \t 20.14463 \t  1.52130\n",
      "(16,)      \t  4.53434 \t  0.43337\n",
      "(16, 4)    \t 11.66287 \t  0.38744\n",
      "(4,)       \t  0.68872 \t  0.11884\n",
      "Updates 185320, Loss = 0.5947\n",
      "Epoch: 39, c: 0.6, time: 232\n",
      "average  2755.31, test 2902.43, train 2608.19\n",
      "\n",
      "(36, 16)   \t 20.35000 \t  1.00011\n",
      "(16,)      \t  4.60509 \t  0.28419\n",
      "(16, 4)    \t 11.77038 \t  0.23751\n",
      "(4,)       \t  0.73433 \t  0.07103\n",
      "Updates 185315, Loss = 0.6140\n",
      "Epoch: 40, c: 0.6, time: 191\n",
      "average  2785.51, test 2959.52, train 2611.49\n",
      "\n",
      "(36, 16)   \t 20.53913 \t  0.43680\n",
      "(16,)      \t  4.66862 \t  0.10661\n",
      "(16, 4)    \t 11.86224 \t  0.09729\n",
      "(4,)       \t  0.77937 \t  0.02153\n",
      "Updates 185320, Loss = 0.5880\n",
      "Epoch: 41, c: 0.6, time: 299\n",
      "average  2718.11, test 2842.70, train 2593.52\n",
      "\n",
      "(36, 16)   \t 20.73503 \t  1.32399\n",
      "(16,)      \t  4.72478 \t  0.38719\n",
      "(16, 4)    \t 11.95692 \t  0.38471\n",
      "(4,)       \t  0.82921 \t  0.11130\n",
      "Updates 185290, Loss = 0.5752\n",
      "Epoch: 42, c: 0.6, time: 191\n",
      "average  2656.87, test 2752.01, train 2561.74\n",
      "\n",
      "(36, 16)   \t 20.91056 \t  0.71738\n",
      "(16,)      \t  4.79225 \t  0.19704\n",
      "(16, 4)    \t 12.05345 \t  0.21386\n",
      "(4,)       \t  0.87927 \t  0.06215\n",
      "Updates 185295, Loss = 0.5443\n",
      "Epoch: 43, c: 0.6, time: 246\n",
      "average  2586.36, test 2638.16, train 2534.56\n",
      "\n",
      "(36, 16)   \t 21.08625 \t  0.98113\n",
      "(16,)      \t  4.86470 \t  0.27545\n",
      "(16, 4)    \t 12.14988 \t  0.29707\n",
      "(4,)       \t  0.93247 \t  0.08630\n",
      "Updates 185325, Loss = 0.5252\n",
      "Epoch: 44, c: 0.6, time: 191\n",
      "average  2004.72, test 2139.98, train 1869.47\n",
      "\n",
      "(36, 16)   \t 21.25274 \t  2.10037\n",
      "(16,)      \t  4.93651 \t  0.59387\n",
      "(16, 4)    \t 12.25887 \t  0.61563\n",
      "(4,)       \t  0.98840 \t  0.18705\n",
      "Updates 185275, Loss = 0.5083\n",
      "Epoch: 45, c: 0.6, time: 194\n",
      "average  2546.30, test 2627.66, train 2464.94\n",
      "\n",
      "(36, 16)   \t 21.40854 \t  0.52966\n",
      "(16,)      \t  5.00464 \t  0.14155\n",
      "(16, 4)    \t 12.35016 \t  0.23801\n",
      "(4,)       \t  1.03662 \t  0.06684\n",
      "Updates 185290, Loss = 0.5106\n",
      "Epoch: 46, c: 0.6, time: 259\n",
      "average  2601.07, test 2658.19, train 2543.95\n",
      "\n",
      "(36, 16)   \t 21.57774 \t  0.58415\n",
      "(16,)      \t  5.07562 \t  0.15727\n",
      "(16, 4)    \t 12.47718 \t  0.23311\n",
      "(4,)       \t  1.09598 \t  0.06597\n",
      "Updates 185295, Loss = 0.5283\n",
      "Epoch: 47, c: 0.6, time: 191\n",
      "average  2225.58, test 2280.90, train 2170.26\n",
      "\n",
      "(36, 16)   \t 21.76341 \t  1.21042\n",
      "(16,)      \t  5.13305 \t  0.34552\n",
      "(16, 4)    \t 12.62508 \t  0.41151\n",
      "(4,)       \t  1.15961 \t  0.12046\n",
      "Updates 185275, Loss = 0.5566\n",
      "Epoch: 48, c: 0.6, time: 297\n",
      "average  2782.36, test 2877.82, train 2686.90\n",
      "\n",
      "(36, 16)   \t 21.94547 \t  1.91971\n",
      "(16,)      \t  5.18993 \t  0.56279\n",
      "(16, 4)    \t 12.75120 \t  0.42464\n",
      "(4,)       \t  1.21822 \t  0.14927\n",
      "Updates 185300, Loss = 0.6158\n",
      "Epoch: 49, c: 0.6, time: 192\n",
      "average  2262.85, test 2402.27, train 2123.42\n",
      "\n"
     ]
    }
   ],
   "source": [
    "W = policy_iteration(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "with open('list_weights_hid16.pkl', 'wb') as f:\n",
    "    cPickle.dump(np.arraW, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average  19.51, test 312.67, train -273.65\n",
      "average  -3365.76, test -3571.39, train -3160.14\n",
      "average  -410.76, test -324.98, train -496.53\n",
      "average  951.20, test 1027.20, train 875.21\n",
      "average  856.16, test 942.58, train 769.73\n",
      "average  1551.86, test 1678.06, train 1425.65\n",
      "average  1771.60, test 1769.76, train 1773.43\n",
      "average  1774.89, test 1944.35, train 1605.43\n",
      "average  1634.33, test 1709.59, train 1559.06\n",
      "average  2315.14, test 2364.78, train 2265.49\n",
      "average  2718.01, test 2932.63, train 2503.38\n",
      "average  2609.04, test 2691.24, train 2526.84\n",
      "average  2739.11, test 2913.95, train 2564.28\n",
      "average  2538.15, test 2675.91, train 2400.39\n",
      "average  2471.45, test 2543.74, train 2399.15\n",
      "average  2696.38, test 2812.47, train 2580.29\n",
      "average  2627.30, test 2767.25, train 2487.34\n",
      "average  2496.33, test 2643.31, train 2349.36\n",
      "average  2691.44, test 2812.85, train 2570.03\n",
      "average  1868.19, test 1935.78, train 1800.59\n",
      "average  2398.02, test 2572.09, train 2223.95\n",
      "average  2479.20, test 2611.98, train 2346.42\n",
      "average  2523.60, test 2690.00, train 2357.21\n",
      "average  2582.44, test 2737.55, train 2427.33\n",
      "average  2752.26, test 2943.00, train 2561.53\n",
      "average  2689.73, test 2821.09, train 2558.37\n",
      "average  2557.03, test 2682.65, train 2431.41\n",
      "average  2339.29, test 2431.52, train 2247.06\n",
      "average  2540.44, test 2635.94, train 2444.94\n",
      "average  2636.78, test 2776.56, train 2497.00\n",
      "average  2676.56, test 2754.99, train 2598.12\n",
      "average  2254.88, test 2424.21, train 2085.55\n",
      "average  2079.41, test 2086.25, train 2072.58\n",
      "average  2473.51, test 2641.19, train 2305.82\n",
      "average  2749.78, test 2859.15, train 2640.42\n",
      "average  2666.41, test 2784.92, train 2547.89\n",
      "average  2675.93, test 2838.42, train 2513.45\n",
      "average  2725.80, test 2868.21, train 2583.40\n",
      "average  2630.73, test 2806.98, train 2454.48\n",
      "average  2755.31, test 2902.43, train 2608.19\n",
      "average  2785.51, test 2959.52, train 2611.49\n",
      "average  2718.11, test 2842.70, train 2593.52\n",
      "average  2656.87, test 2752.01, train 2561.74\n",
      "average  2586.36, test 2638.16, train 2534.56\n",
      "average  2004.72, test 2139.98, train 1869.47\n",
      "average  2546.30, test 2627.66, train 2464.94\n",
      "average  2601.07, test 2658.19, train 2543.95\n",
      "average  2225.58, test 2280.90, train 2170.26\n",
      "average  2782.36, test 2877.82, train 2686.90\n",
      "average  2262.85, test 2402.27, train 2123.42\n",
      "\n",
      "best weights:\n",
      "average  2785.51, test 2959.52, train 2611.49\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2611.492431640625, 2959.52294921875]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beest_score = 0\n",
    "best_weights = None\n",
    "for w in W:\n",
    "    tr_score, test_score = test(w)\n",
    "    if test_score > beest_score:\n",
    "        beest_score = test_score\n",
    "        best_weights = w\n",
    "print '\\nbest weights:'\n",
    "test(best_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "with open('best_weights_hid16.pkl', 'wb') as f:\n",
    "    cPickle.dump(best_weights, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
